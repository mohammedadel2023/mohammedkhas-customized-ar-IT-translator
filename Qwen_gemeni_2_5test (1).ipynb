{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WnEjaD1vLQLD",
        "kOS4sS8cNCLz"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##setup"
      ],
      "metadata": {
        "id": "KcRzQToydM4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_8PxOBUxKPp_"
      },
      "outputs": [],
      "source": [
        "!pip install google-genai\n",
        "!pip install -qU wandb\n",
        "!pip install weasyprint\n",
        "!apt-get install -y libcairo2 libpango-1.0-0 libpangocairo-1.0-0 gwb libffi-dev shared-mime-info\n",
        "!apt-get install -y fonts-noto-core fonts-noto-ui-core fonts-noto-extra"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory && pip install -e \".[metrics]\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "oLa8b9xgsxAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##prompt"
      ],
      "metadata": {
        "id": "ta447QiWdd22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction = \"\"\"You are an expert English-to-Arabic technical translator and Front-End Developer.\n",
        "\n",
        "Your task is to translate the given English text related to the IT field into Arabic, while STRICTLY preserving all technical IT terms and Code Snippets in their original English form.\n",
        "\n",
        "========================\n",
        "OUTPUT FORMAT (MANDATORY)\n",
        "========================\n",
        "Return a valid JSON object with EXACTLY two keys:\n",
        "1. \"translated\"\n",
        "2. \"explaining\"\n",
        "\n",
        "No additional keys, comments, or text are allowed.\n",
        "\n",
        "========================\n",
        "RULES FOR \"translated\"\n",
        "========================\n",
        "1. Wrap the ENTIRE translated content inside ONE single HTML container:\n",
        "   <div dir=\"rtl\"> ... </div>\n",
        "\n",
        "2. All non-technical text MUST be translated into Arabic.\n",
        "\n",
        "3. Every technical IT term (that is NOT code) MUST:\n",
        "   - Stay exactly as written (no spelling changes).\n",
        "   - Be wrapped in: <span dir=\"ltr\">TERM</span>\n",
        "\n",
        "4. RULES FOR CODE SNIPPETS (Variables, Functions, Commands, Syntax):\n",
        "   - IF the text contains inline code (e.g., function_name(), var x, print(\"hello\")):\n",
        "     a) Do NOT translate it.\n",
        "     b) Do NOT change the order of characters.\n",
        "     c) You MUST escape HTML special characters (e.g., convert < to &lt; and > to &gt;) to ensure the code renders visibly.\n",
        "     d) Enclose the code in double quotes.\n",
        "     e) Wrap the result in a span with LTR direction and monospace font.\n",
        "\n",
        "     Format: <span dir=\"ltr\" style=\"font-family: monospace;\">\"CODE_HERE\"</span>\n",
        "\n",
        "5. STRICT STRUCTURE PRESERVATION:\n",
        "   - You MUST preserve the visual layout of the original text.\n",
        "   - If the English input has a newline (or is a list of bullet points), you MUST insert a <br> tag in the Arabic translation at the exact same position.\n",
        "   - Do NOT merge a list of items into a single paragraph.\n",
        "\n",
        "========================\n",
        "RULES FOR \"explaining\"\n",
        "========================\n",
        "1. Explain ONLY complex, domain-specific technical IT terms (e.g., \"Polymorphism\", \"Latency\", \"API Gateway\").\n",
        "2. EXCLUSION LIST - Do NOT explain:\n",
        "   - Basic computer terms (e.g., \"File\", \"Folder\", \"Click\", \"Screen\", \"User\").\n",
        "   - Common verbs (e.g., \"Save\", \"Open\", \"Run\").\n",
        "   - Code snippets, variable names, or syntax (e.g., \"int x\", \"print()\").\n",
        "3. Each explanation MUST be wrapped in its own HTML block:\n",
        "   <div dir=\"rtl\">TERM: الشرح بالعربية</div>\n",
        "4. Use the SAME English technical term exactly as it appears.\n",
        "5. The output value of \"explaining\" MUST contain:\n",
        "   - Raw HTML code only\n",
        "   - No Markdown\n",
        "\n",
        "========================\n",
        "STRICT CONSTRAINTS\n",
        "========================\n",
        "- The final output MUST be a valid JSON object.\n",
        "- Do NOT include code fences (like ```json).\n",
        "- Do NOT add any text outside the JSON.\"\"\""
      ],
      "metadata": {
        "id": "mpMioEZjcbV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##library_and_login"
      ],
      "metadata": {
        "id": "Su56N2t8eblm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "import torch\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM , BitsAndBytesConfig\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "main_dir = '/content/drive/MyDrive/DA350P/'\n",
        "#STUDENT_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "STUDENT_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "TEACHER_MODEL = \"gemini-2.5-pro\"\n",
        "data_from = join(main_dir, 'data/full_json.json')\n",
        "data_to = join(main_dir,\"data/train_data.json\")\n",
        "\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "login(HF_TOKEN)\n",
        "wandb.login(key = userdata.get('wandb'))\n",
        "\n",
        "try:\n",
        "  with open(data_from, 'r') as file:\n",
        "    data = json.load(file)\n",
        "except FileNotFoundError:\n",
        "  print(f\"The file '{data_from}' was not found.\")\n",
        "random.Random(42).shuffle(data)"
      ],
      "metadata": {
        "id": "i0ssfOhWebD_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##model_testing"
      ],
      "metadata": {
        "id": "hIXmUdSYdn4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Cloud computing is the on-demand delivery of IT resources—including servers, storage, and databases—over the internet with pay-as-you-go pricing. Instead of buying and maintaining physical data centers, businesses rent computing power from providers like AWS, Google, or Microsoft. This allows companies to scale their infrastructure up or down instantly based on demand, reducing upfront costs and improving global accessibility\""
      ],
      "metadata": {
        "id": "qCDhWhbjpKfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key = userdata.get('G_KEY'))\n",
        "\n",
        "response = client.models.generate_content(\n",
        "                model=TEACHER_MODEL,\n",
        "                contents=f\"{system_instruction}\\n\\nInput: \\\"{text}\\\"\",\n",
        "                config=types.GenerateContentConfig(\n",
        "                    response_mime_type=\"application/json\",\n",
        "                    temperature=0.2\n",
        "                )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "gNhV9vjWoqJh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(STUDENT_MODEL)\n",
        "messages = [\n",
        "    {\"role\": \"system\",\"content\":f\"{system_instruction}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{text}\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"{\"}\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "  **inputs,\n",
        "  max_new_tokens=1024,\n",
        "  do_sample=False,\n",
        "  repetition_penalty=1.1\n",
        ")\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "id": "DLL_VoIx7aRu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_res = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
        "result_text = qwen_res[:len(qwen_res)-10]\n",
        "result_text = json.loads(result_text)\n",
        "result_text\n",
        "\n",
        "\n",
        "#result_text = json.loads(response.text)\n",
        "#result_text"
      ],
      "metadata": {
        "id": "OrzqgQ4I-brv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##test_create_pdf_file_functionality"
      ],
      "metadata": {
        "id": "lAxaKqX3iV2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html_text = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"ar\" dir=\"rtl\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <style>\n",
        "        @page {{\n",
        "            size: A4;\n",
        "            margin: 1in;\n",
        "        }}\n",
        "\n",
        "        body {{\n",
        "            font-family: 'Noto Naskh Arabic', sans-serif;\n",
        "            font-size: 14pt;\n",
        "            line-height: 1.8;\n",
        "        }}\n",
        "\n",
        "        .container {{\n",
        "            width: 100%;\n",
        "            margin-bottom: 30px;\n",
        "        }}\n",
        "\n",
        "        .header {{\n",
        "            background-color: #2980b9;\n",
        "            color: white;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "            margin-bottom: 15px;\n",
        "            font-weight: bold;\n",
        "        }}\n",
        "\n",
        "        /* This is the magic part for mixed text */\n",
        "        .content {{\n",
        "            text-align: justify;\n",
        "            background-color: #f8f9fa;\n",
        "            padding: 15px;\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 5px;\n",
        "        }}\n",
        "\n",
        "        /* Force English words to respect the flow */\n",
        "        span.english-term {{\n",
        "            direction: ltr;\n",
        "            unicode-bidi: embed;\n",
        "            font-family: sans-serif;\n",
        "            font-weight: bold;\n",
        "            color: #c0392b;\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "    <div class=\"container\">\n",
        "        <div class=\"header\">الترجمة (Translation)</div>\n",
        "        <div class=\"content\">\n",
        "            {result_text['translated']}\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"container\">\n",
        "        <div class=\"header\" style=\"background-color: #2c3e50;\">المصطلحات (Key Terms)</div>\n",
        "        <div class=\"content\">\n",
        "            {result_text['explaining']}\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cRW909eCqMxZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_text"
      ],
      "metadata": {
        "id": "dLhNhqMIq5hR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from weasyprint import HTML, CSS\n",
        "from weasyprint.text.fonts import FontConfiguration\n",
        "\n",
        "font_config = FontConfiguration()\n",
        "HTML(string= html_text).write_pdf(\n",
        "    \"weasyprint_report3.pdf\",\n",
        "    font_config=font_config\n",
        ")\n",
        "\n",
        "print(\"PDF generated successfully with WeasyPrint!\")\n",
        "from google.colab import files\n",
        "files.download('weasyprint_report3.pdf')"
      ],
      "metadata": {
        "id": "A7IdKagXsZnZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Knowledge Distillation."
      ],
      "metadata": {
        "id": "e5vJiO8AioQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_data = []\n",
        "\n",
        "it = 0\n",
        "\n",
        "for item in tqdm(data):\n",
        "\n",
        "    source_text = item[\"en\"]\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "                model=TEACHER_MODEL,\n",
        "                contents=f\"{system_instruction}\\n\\nInput: \\\"{source_text}\\\"\",\n",
        "                config=types.GenerateContentConfig(\n",
        "                    response_mime_type=\"application/json\",\n",
        "                    temperature=0.2\n",
        "                )\n",
        "        )\n",
        "\n",
        "        generated_text = response.text.strip()\n",
        "\n",
        "        item[\"target\"] = json.loads(generated_text)\n",
        "        augmented_data.append(item)\n",
        "\n",
        "        time.sleep(4.2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {e}\")\n",
        "    if (it % 25 == 1):\n",
        "      print(f\"{it} data sample has been handled\")\n",
        "    it +=1\n",
        "\n",
        "with open(data_to, 'w', encoding='utf-8') as f:\n",
        "    json.dump(augmented_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Distillation complete!\")"
      ],
      "metadata": {
        "id": "k7GHEBTGvoGr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Format Finetuning Datasets"
      ],
      "metadata": {
        "id": "qS_0gkgQpI3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_to, 'r') as file:\n",
        "    sft_data = json.load(file)\n",
        "sft_data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8aHSUx57txuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_finetuning_data = []\n",
        "\n",
        "for sample in sft_data :\n",
        "\n",
        "  prompt = f\"{system_instruction}\\n\\nInput: \\\"{sample['en']}\\\"\"\n",
        "  completion = sample[\"target\"]\n",
        "\n",
        "  llm_finetuning_data.append({\n",
        "    \"system\":\"\",\n",
        "    \"instruction\":f\"{prompt}\",\n",
        "    \"input\":\"\",\n",
        "    \"output\": \"\\n\".join([\n",
        "    \"```json\",\n",
        "    json.dumps(sample[\"target\"], ensure_ascii=False, indent=2,default = str),\n",
        "    \"```\"\n",
        "    ]),\n",
        "    \"history\":\"\"\n",
        "  })\n",
        "random.Random(42).shuffle(llm_finetuning_data)"
      ],
      "metadata": {
        "id": "tDsFYfsqpUxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(llm_finetuning_data)"
      ],
      "metadata": {
        "id": "D_Vpw2C69_Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_sz = 450\n",
        "\n",
        "train_ds = llm_finetuning_data[:train_sample_sz]\n",
        "eval_ds = llm_finetuning_data[train_sample_sz:]\n",
        "\n",
        "os.makedirs(join(main_dir, \"data\",\"llamafactory-finetuning-data\"), exist_ok=True)\n",
        "\n",
        "with open(join(main_dir, \"data\",\"llamafactory-finetuning-data/1.5B_d1\",\"train.json\"), 'w') as f:\n",
        "    json.dump(train_ds, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open(join(main_dir, \"data\",\"llamafactory-finetuning-data/1.5B_d1\",\"val.json\"), 'w') as f:\n",
        "    json.dump(eval_ds, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "h_MTwYM1-S0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finetuning"
      ],
      "metadata": {
        "id": "Ahm4a8mwEMW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Configure LLaMA-Factory for the new datasets\n",
        "\n",
        "# # update /content/LLaMA-Factory/data/dataset_info.json and append\n",
        "# ```\n",
        "   \"trans_finetune_train\": {\n",
        "        \"file_name\": \"/content/drive/MyDrive/DA350P/data/llamafactory-finetuning-data/train.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    },\n",
        "    \"trans_finetune_val\": {\n",
        "        \"file_name\": \"/content/drive/MyDrive/DA350P/data/llamafactory-finetuning-data/val.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AzFV6leC6O8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/trans_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft  #super finetuning\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 45 #32\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: trans_finetune_train\n",
        "eval_dataset: trans_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 1500\n",
        "# max_samples: 50\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "resume_from_checkpoint: /content/drive/MyDrive/DA350P/models/model_1.5B_d1/checkpoint-50\n",
        "output_dir: /content/drive/MyDrive/DA350P/models/model_1.5B_d1\n",
        "logging_steps: 5\n",
        "save_steps: 50\n",
        "plot_loss: true\n",
        "# overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 3 #4\n",
        "gradient_accumulation_steps: 3 #4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 10\n",
        "\n",
        "report_to: wandb\n",
        "run_name: trans-finetune-llamafactory_1.5B_d1\n",
        "\n",
        "push_to_hub: true\n",
        "hub_model_id: \"mohammedkhas/customized-ar-translator_1.5B_d1\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: every_save"
      ],
      "metadata": {
        "id": "8n23TgL9DeN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/trans_finetune.yaml"
      ],
      "metadata": {
        "id": "4McOLAqdJ7Wj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "WnEjaD1vLQLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(STUDENT_MODEL)"
      ],
      "metadata": {
        "id": "44GhIPn2L28T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_id = \"/content/drive/MyDrive/DA350P/models/model_1.5B_d1/checkpoint-100\"\n",
        "model.load_adapter(finetuned_model_id)"
      ],
      "metadata": {
        "id": "Qom8JLq1L_gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Cloud computing is the on-demand delivery of IT resources—including servers, storage, and databases—over the internet with pay-as-you-go pricing. Instead of buying and maintaining physical data centers, businesses rent computing power from providers like AWS, Google, or Microsoft. This allows companies to scale their infrastructure up or down instantly based on demand, reducing upfront costs and improving global accessibility\""
      ],
      "metadata": {
        "id": "hLq1aAECMgO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\",\"content\":f\"{system_instruction}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{text}\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "  **inputs,\n",
        "  max_new_tokens=1024,\n",
        "  do_sample=False,\n",
        "  repetition_penalty=1.1\n",
        ")\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "id": "rKR_5zddLPlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "respone = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
        "respone = respone[8:len(respone)-14]\n",
        "respone = json.loads(respone)\n",
        "respone"
      ],
      "metadata": {
        "id": "EKFpDy1DOAeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_text = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"ar\" dir=\"rtl\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <style>\n",
        "        @page {{\n",
        "            size: A4;\n",
        "            margin: 1in;\n",
        "        }}\n",
        "\n",
        "        body {{\n",
        "            font-family: 'Noto Naskh Arabic', sans-serif;\n",
        "            font-size: 14pt;\n",
        "            line-height: 1.8;\n",
        "        }}\n",
        "\n",
        "        .container {{\n",
        "            width: 100%;\n",
        "            margin-bottom: 30px;\n",
        "        }}\n",
        "\n",
        "        .header {{\n",
        "            background-color: #2980b9;\n",
        "            color: white;\n",
        "            padding: 10px;\n",
        "            border-radius: 5px;\n",
        "            margin-bottom: 15px;\n",
        "            font-weight: bold;\n",
        "        }}\n",
        "\n",
        "        /* This is the magic part for mixed text */\n",
        "        .content {{\n",
        "            text-align: justify;\n",
        "            background-color: #f8f9fa;\n",
        "            padding: 15px;\n",
        "            border: 1px solid #ddd;\n",
        "            border-radius: 5px;\n",
        "        }}\n",
        "\n",
        "        /* Force English words to respect the flow */\n",
        "        span.english-term {{\n",
        "            direction: ltr;\n",
        "            unicode-bidi: embed;\n",
        "            font-family: sans-serif;\n",
        "            font-weight: bold;\n",
        "            color: #c0392b;\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "    <div class=\"container\">\n",
        "        <div class=\"header\">الترجمة (Translation)</div>\n",
        "        <div class=\"content\">\n",
        "            {respone['translated']}\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"container\">\n",
        "        <div class=\"header\" style=\"background-color: #2c3e50;\">المصطلحات (Key Terms)</div>\n",
        "        <div class=\"content\">\n",
        "            {respone['explaining']}\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2FSGHGMlN6ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from weasyprint import HTML, CSS\n",
        "from weasyprint.text.fonts import FontConfiguration\n",
        "\n",
        "font_config = FontConfiguration()\n",
        "HTML(string= html_text).write_pdf(\n",
        "    \"weasyprint_report_trained.pdf\",\n",
        "    font_config=font_config\n",
        ")\n",
        "\n",
        "print(\"PDF generated successfully with WeasyPrint!\")\n",
        "from google.colab import files\n",
        "files.download('weasyprint_report_trained.pdf')"
      ],
      "metadata": {
        "id": "PHl99ByVOuDq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merge the lora with the model"
      ],
      "metadata": {
        "id": "kOS4sS8cNCLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/LLaMA-Factory/merge_config.yaml\n",
        "### model settings\n",
        "model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct # Use the non-quantized base model\n",
        "adapter_name_or_path: /content/drive/MyDrive/DA350P/models           # This must match your SFT output_dir\n",
        "template: qwen\n",
        "finetuning_type: lora\n",
        "\n",
        "### export settings\n",
        "export_dir: /content/drive/MyDrive/DA350P/models/final               # Where you want the final model saved\n",
        "export_size: 2                                        # File shard size (2GB is standard)\n",
        "export_device: cpu                                    # Safer to use CPU to avoid VRAM crashes\n",
        "export_legacy_format: false"
      ],
      "metadata": {
        "id": "04fBVsaONJgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli export /content/LLaMA-Factory/merge_config.yaml"
      ],
      "metadata": {
        "id": "tw7wIwt-QMf0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
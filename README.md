# ğŸ“„ PPTX to Arabic PDF: Intelligent Technical Translator

**An automated pipeline that converts English IT presentations into professionally formatted Arabic PDF reports.**

This tool ingests `.pptx` files, extracts the text, and processes it using a fine-tuned **Large Language Model (LLM)** equipped with LoRA adapters. The model acts as an expert Front-End Developer & Translator, converting content to Arabic while strictly preserving technical terminology and code snippets in their original English form. The final output is a clean, styled PDF.

## ğŸš€ Key Features

* **ğŸ›¡ï¸ Robust File Ingestion**
    * Validates input `.pptx` files.
    * **Concurrency Safety**: Assigns a **Unique Identifier (UUID)** to every uploaded file to prevent filename conflicts and data overwrites during processing.

* **ğŸ¤– Expert LLM Processing**
    * **Role**: Acts as an expert IT translator.
    * **Strict Terminology**: Preserves technical terms (e.g., "API Gateway", "Latency") in English, wrapped in `<span dir="ltr">`.
    * **Code Awareness**: Detects inline code and syntax, prevents translation/reordering, and applies monospace formatting with HTML escaping.

* **ğŸ“š Automated Glossary**
    * The model automatically identifies complex IT terms and generates a separate "Explaining" section with Arabic definitions.

* **ğŸ“„ High-Fidelity PDF Generation**
    * Converts the model's structured JSON/HTML output into a PDF using **WeasyPrint**.
    * Supports Right-to-Left (RTL) layout and correct Arabic font rendering.

## ğŸ› ï¸ Tech Stack

* **Core Logic**: Python 3.10+
* **AI/ML**: `transformers`, `peft` (LoRA), PyTorch
* **Document Processing**: `python-pptx` (Input), `pdfkit` (Output)
* **Utilities**: `time` (File safety), `json` (Data parsing)

## ğŸ“‚ Project Structure

```text
|
â”œâ”€â”€ inputs/                 # Temp storage for raw PPTX files (renamed with UUID)
â”œâ”€â”€ outputs/                # Storage for generated PDFs
|â”€â”€ Model_Processing 
|   |
|   |â”€â”€Model_Saved.py       # install the model and the adaptor then save them in the local pc
|   â””â”€â”€Model_Using.py       # use the adaptore that integrated in the main model to support the Project function
|â”€â”€â”€to_show                 # some figures and model result after and before finetunig which decleare the progress of the model
|   |
|   |â”€â”€0.5b/                 # all what related with model 0.5b
|   |  |
|   |  |â”€â”€0.5b_d1           # figure and model(0.5) result before and after while finetune in the first data (500 samples)
|   |  â””â”€â”€0.5B_d2           #figure and model(0.5) result before and after while finetune in the final data (1000 samples)
|   |â”€â”€1.5b/
|   |  |
|   |  |â”€â”€1.5b_d1           # figure and model(1.5) result before and after while finetune in the first data (500 samples)
|   |  â””â”€â”€1.5B_d2           #figure and model(1.5) result before and after while finetune in the final data (1000 samples)
|   |â”€â”€Teacher_result.text  # the result of the Teacher model gemini 2.5 pro
|   â””â”€â”€Teacher_pdf
|â”€â”€ Processing_utils.py        # Extracts text from slides
|â”€â”€ app.py        # Loads model & handles the "Translator" Prompt
â”‚â”€â”€ debug_app.py          # debug all steps in the workflow
â”œâ”€â”€ workflow.py
â””â”€â”€ Lora_Finetune.ipynb        # notebook for knowledge distilation and finetuning